% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate_classifier_performance.R
\name{evaluate_classifier_performance}
\alias{evaluate_classifier_performance}
\title{Evaluate Auto-Tuned Classifier Performance}
\usage{
evaluate_classifier_performance(
  model_results,
  data = NULL,
  decision_threshold = 0.5
)
}
\arguments{
\item{model_results}{Output from auto_tune_classifier function (list or named list of learners)}

\item{data}{List with two elements: X_validate (features) and Y_validate (outcomes).
If NULL (default), uses training data from the learners.}

\item{decision_threshold}{Numeric threshold for converting probabilities to class predictions (default: 0.5)}
}
\value{
Data frame with performance metrics for each model, including improvement calculations
        when both tuned and untuned models are present
}
\description{
This function evaluates the performance of models returned by auto_tune_classifier
using comprehensive binary classification metrics.
}
\examples{
\dontrun{
# Evaluate with validation data
validation_data <- list(X_validate = X_val, Y_validate = Y_val)
results <- evaluate_classifier_performance(model_results, validation_data)

# Evaluate with training data
results <- evaluate_classifier_performance(model_results)
}

}
